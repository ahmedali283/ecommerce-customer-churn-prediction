{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Customer Churn Prediction - Model Training\n",
    "## Training and Comparing Multiple ML Models\n",
    "\n",
    "This notebook trains 7 different machine learning models and compares their performance to find the best churn predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Import Required Libraries\n",
    "Import all necessary libraries for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "\n",
    "# Handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Model tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# Model persistence\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Preprocessed Data\n",
    "Load the train and test datasets created during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\nTarget Distribution in Training Set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"  Class {val}: {count:,} ({count/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Handle Class Imbalance with SMOTE\n",
    "Apply SMOTE to balance the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Handling Class Imbalance with SMOTE...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Before SMOTE:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"  Class {val}: {count:,}\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "unique, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"  Class {val}: {count:,}\")\n",
    "\n",
    "print(f\"\\nTraining set size increased from {len(X_train):,} to {len(X_train_balanced):,}\")\n",
    "print(\"‚úì Class imbalance handled!\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "unique_before, counts_before = np.unique(y_train, return_counts=True)\n",
    "axes[0].bar(['Not Churned', 'Churned'], counts_before, color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Class Distribution - Before SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "for i, v in enumerate(counts_before):\n",
    "    axes[0].text(i, v + 500, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# After SMOTE\n",
    "unique_after, counts_after = np.unique(y_train_balanced, return_counts=True)\n",
    "axes[1].bar(['Not Churned', 'Churned'], counts_after, color=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('Class Distribution - After SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "for i, v in enumerate(counts_after):\n",
    "    axes[1].text(i, v + 500, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Define Model Evaluation Function\n",
    "Create a comprehensive function to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Prediction probabilities (for ROC-AUC)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_test_proba = model.decision_function(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Training_Time': training_time,\n",
    "        'Train_Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'Test_Accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'Precision': precision_score(y_test, y_test_pred),\n",
    "        'Recall': recall_score(y_test, y_test_pred),\n",
    "        'F1_Score': f1_score(y_test, y_test_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_test, y_test_proba)\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTraining Time: {training_time:.2f} seconds\")\n",
    "    print(f\"\\nTraining Accuracy: {metrics['Train_Accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['Test_Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1_Score']:.4f}\")\n",
    "    print(f\"ROC-AUC Score: {metrics['ROC_AUC']:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['Not Churned', 'Churned']))\n",
    "    \n",
    "    return metrics, model, y_test_pred, y_test_proba\n",
    "\n",
    "print(\"‚úì Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Model 1 - Logistic Regression\n",
    "Train a baseline Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
    "\n",
    "# Train and evaluate\n",
    "lr_metrics, lr_trained, lr_pred, lr_proba = evaluate_model(\n",
    "    lr_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "cm = confusion_matrix(y_test, lr_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Logistic Regression completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Model 2 - Decision Tree\n",
    "Train a Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    random_state=42, \n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "dt_metrics, dt_trained, dt_pred, dt_proba = evaluate_model(\n",
    "    dt_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"Decision Tree\"\n",
    ")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "cm = confusion_matrix(y_test, dt_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('Decision Tree - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Decision Tree completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Model 3 - Random Forest\n",
    "Train a Random Forest ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "rf_metrics, rf_trained, rf_pred, rf_proba = evaluate_model(\n",
    "    rf_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"Random Forest\"\n",
    ")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_trained.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Random Forest - Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('Random Forest - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Random Forest completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Model 4 - XGBoost\n",
    "Train an XGBoost gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "xgb_metrics, xgb_trained, xgb_pred, xgb_proba = evaluate_model(\n",
    "    xgb_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"XGBoost\"\n",
    ")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_trained.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='rocket')\n",
    "plt.title('XGBoost - Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('XGBoost - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì XGBoost completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Model 5 - LightGBM\n",
    "Train a LightGBM gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "lgb_metrics, lgb_trained, lgb_pred, lgb_proba = evaluate_model(\n",
    "    lgb_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"LightGBM\"\n",
    ")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgb_trained.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='mako')\n",
    "plt.title('LightGBM - Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, lgb_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('LightGBM - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì LightGBM completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Model 6 - Gradient Boosting\n",
    "Train a Scikit-learn Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "gb_metrics, gb_trained, gb_pred, gb_proba = evaluate_model(\n",
    "    gb_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"Gradient Boosting\"\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, gb_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdPu', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('Gradient Boosting - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Gradient Boosting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Model 7 - Neural Network (MLP)\n",
    "Train a Multi-Layer Perceptron neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50, 25),\n",
    "    random_state=42,\n",
    "    max_iter=300,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "mlp_metrics, mlp_trained, mlp_pred, mlp_proba = evaluate_model(\n",
    "    mlp_model, X_train_balanced, y_train_balanced, X_test, y_test, \n",
    "    \"Neural Network (MLP)\"\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, mlp_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('Neural Network - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Neural Network completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Compare All Models\n",
    "Create a comprehensive comparison of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all metrics\n",
    "all_metrics = pd.DataFrame([\n",
    "    lr_metrics,\n",
    "    dt_metrics,\n",
    "    rf_metrics,\n",
    "    xgb_metrics,\n",
    "    lgb_metrics,\n",
    "    gb_metrics,\n",
    "    mlp_metrics\n",
    "])\n",
    "\n",
    "# Sort by F1 Score (balanced metric for imbalanced data)\n",
    "all_metrics_sorted = all_metrics.sort_values('F1_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON - ALL METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(all_metrics_sorted.to_string(index=False))\n",
    "\n",
    "# Highlight best model\n",
    "best_model = all_metrics_sorted.iloc[0]\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"üèÜ BEST MODEL: {best_model['Model']}\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Test Accuracy: {best_model['Test_Accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_model['Precision']:.4f}\")\n",
    "print(f\"Recall: {best_model['Recall']:.4f}\")\n",
    "print(f\"F1 Score: {best_model['F1_Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {best_model['ROC_AUC']:.4f}\")\n",
    "print(f\"Training Time: {best_model['Training_Time']:.2f} seconds\")\n",
    "\n",
    "# Save comparison\n",
    "all_metrics_sorted.to_csv('model_comparison.csv', index=False)\n",
    "print(\"\\n‚úì Model comparison saved to 'model_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: Visualize Model Comparison\n",
    "Create visual comparisons of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "x_pos = np.arange(len(all_metrics_sorted))\n",
    "axes[0, 0].barh(x_pos, all_metrics_sorted['Test_Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_yticks(x_pos)\n",
    "axes[0, 0].set_yticklabels(all_metrics_sorted['Model'])\n",
    "axes[0, 0].set_xlabel('Accuracy Score', fontsize=12)\n",
    "axes[0, 0].set_title('Model Comparison - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "for i, v in enumerate(all_metrics_sorted['Test_Accuracy']):\n",
    "    axes[0, 0].text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# 2. F1 Score Comparison\n",
    "axes[0, 1].barh(x_pos, all_metrics_sorted['F1_Score'], color='lightcoral')\n",
    "axes[0, 1].set_yticks(x_pos)\n",
    "axes[0, 1].set_yticklabels(all_metrics_sorted['Model'])\n",
    "axes[0, 1].set_xlabel('F1 Score', fontsize=12)\n",
    "axes[0, 1].set_title('Model Comparison - F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "for i, v in enumerate(all_metrics_sorted['F1_Score']):\n",
    "    axes[0, 1].text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# 3. ROC-AUC Comparison\n",
    "axes[1, 0].barh(x_pos, all_metrics_sorted['ROC_AUC'], color='lightgreen')\n",
    "axes[1, 0].set_yticks(x_pos)\n",
    "axes[1, 0].set_yticklabels(all_metrics_sorted['Model'])\n",
    "axes[1, 0].set_xlabel('ROC-AUC Score', fontsize=12)\n",
    "axes[1, 0].set_title('Model Comparison - ROC-AUC', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].invert_yaxis()\n",
    "for i, v in enumerate(all_metrics_sorted['ROC_AUC']):\n",
    "    axes[1, 0].text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "axes[1, 1].barh(x_pos, all_metrics_sorted['Training_Time'], color='plum')\n",
    "axes[1, 1].set_yticks(x_pos)\n",
    "axes[1, 1].set_yticklabels(all_metrics_sorted['Model'])\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1, 1].set_title('Model Comparison - Training Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "for i, v in enumerate(all_metrics_sorted['Training_Time']):\n",
    "    axes[1, 1].text(v + 0.2, i, f'{v:.2f}s', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Model comparison visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 14: Multi-Metric Comparison\n",
    "Visualize Precision, Recall, and F1 Score together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric comparison\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1_Score']\n",
    "x = np.arange(len(all_metrics_sorted))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "bars1 = ax.bar(x - width, all_metrics_sorted['Precision'], width, label='Precision', color='#3498db')\n",
    "bars2 = ax.bar(x, all_metrics_sorted['Recall'], width, label='Recall', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, all_metrics_sorted['F1_Score'], width, label='F1 Score', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Precision, Recall, and F1 Score Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_metrics_sorted['Model'], rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "add_labels(bars3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Multi-metric comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 15: ROC Curve Comparison\n",
    "Plot ROC curves for all models on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all predictions and probabilities\n",
    "models_data = [\n",
    "    ('Logistic Regression', lr_proba),\n",
    "    ('Decision Tree', dt_proba),\n",
    "    ('Random Forest', rf_proba),\n",
    "    ('XGBoost', xgb_proba),\n",
    "    ('LightGBM', lgb_proba),\n",
    "    ('Gradient Boosting', gb_proba),\n",
    "    ('Neural Network', mlp_proba)\n",
    "]\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple', 'brown', 'pink']\n",
    "\n",
    "for idx, (model_name, y_proba) in enumerate(models_data):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=colors[idx], lw=2, \n",
    "             label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curve Comparison - All Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì ROC curve comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 16: Hyperparameter Tuning for Best Model\n",
    "Perform grid search to optimize the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyperparameter Tuning for Best Model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best model (based on F1 score)\n",
    "best_model_name = all_metrics_sorted.iloc[0]['Model']\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Current F1 Score: {all_metrics_sorted.iloc[0]['F1_Score']:.4f}\\n\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "if 'XGBoost' in best_model_name:\n",
    "    model = xgb.XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 7, 9],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "elif 'LightGBM' in best_model_name:\n",
    "    model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 7, 9],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "elif 'Random Forest' in best_model_name:\n",
    "    model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [5, 10, 15],\n",
    "        'min_samples_leaf': [2, 5, 10]\n",
    "    }\n",
    "elif 'Gradient Boosting' in best_model_name:\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "else:\n",
    "    # Default to XGBoost if model not recognized\n",
    "    model = xgb.XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 7],\n",
    "        'learning_rate': [0.1, 0.2]\n",
    "    }\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "print(f\"Parameter grid: {param_grid}\\n\")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Use a smaller sample for faster tuning (optional)\n",
    "sample_size = min(20000, len(X_train_balanced))\n",
    "X_sample = X_train_balanced[:sample_size]\n",
    "y_sample = y_train_balanced[:sample_size]\n",
    "\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation F1 Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train best model on full data\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "best_tuned_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"\\n‚úì Hyperparameter tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 17: Evaluate Tuned Model\n",
    "Evaluate the performance of the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Tuned Model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_tuned = best_tuned_model.predict(X_test)\n",
    "y_test_proba_tuned = best_tuned_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "tuned_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_test_pred_tuned),\n",
    "    'Precision': precision_score(y_test, y_test_pred_tuned),\n",
    "    'Recall': recall_score(y_test, y_test_pred_tuned),\n",
    "    'F1_Score': f1_score(y_test, y_test_pred_tuned),\n",
    "    'ROC_AUC': roc_auc_score(y_test, y_test_proba_tuned)\n",
    "}\n",
    "\n",
    "print(\"\\nTUNED MODEL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "for metric, value in tuned_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_tuned = confusion_matrix(y_test, y_test_pred_tuned)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_tuned)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_tuned, target_names=['Not Churned', 'Churned']))\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='RdYlGn_r', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title(f'Tuned {best_model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare before and after tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEFORE vs AFTER TUNING\")\n",
    "print(\"=\"*80)\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1_Score', 'ROC_AUC'],\n",
    "    'Before': [\n",
    "        all_metrics_sorted.iloc[0]['Test_Accuracy'],\n",
    "        all_metrics_sorted.iloc[0]['Precision'],\n",
    "        all_metrics_sorted.iloc[0]['Recall'],\n",
    "        all_metrics_sorted.iloc[0]['F1_Score'],\n",
    "        all_metrics_sorted.iloc[0]['ROC_AUC']\n",
    "    ],\n",
    "    'After': list(tuned_metrics.values())\n",
    "})\n",
    "comparison['Improvement'] = comparison['After'] - comparison['Before']\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úì Tuned model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 18: Feature Importance from Best Model\n",
    "Analyze which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tuned model\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_tuned_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTOP 20 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    print(feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualize top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_20 = feature_importance.head(20)\n",
    "    sns.barplot(data=top_20, x='Importance', y='Feature', palette='viridis')\n",
    "    plt.title(f'Top 20 Feature Importances - Tuned {best_model_name}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"\\n‚úì Feature importance saved to 'feature_importance.csv'\")\n",
    "else:\n",
    "    print(\"Feature importance not available for this model.\")\n",
    "\n",
    "print(\"\\n‚úì Feature importance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 19: Save Best Model\n",
    "Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving Best Model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save using joblib (recommended for scikit-learn models)\n",
    "model_filename = f'best_churn_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "joblib.dump(best_tuned_model, model_filename)\n",
    "print(f\"‚úì Model saved as: {model_filename}\")\n",
    "\n",
    "# Save using pickle as backup\n",
    "pickle_filename = f'best_churn_model_{best_model_name.replace(\" \", \"_\").lower()}.pickle'\n",
    "with open(pickle_filename, 'wb') as f:\n",
    "    pickle.dump(best_tuned_model, f)\n",
    "print(f\"‚úì Model also saved as: {pickle_filename}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'best_parameters': grid_search.best_params_,\n",
    "    'metrics': tuned_metrics,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_features': X_train.shape[1],\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(\"‚úì Model metadata saved as: model_metadata.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"F1 Score: {tuned_metrics['F1_Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {tuned_metrics['ROC_AUC']:.4f}\")\n",
    "print(f\"Accuracy: {tuned_metrics['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 20: Load and Test Saved Model\n",
    "Demonstrate how to load and use the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Model Loading...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(model_filename)\n",
    "print(f\"‚úì Model loaded from: {model_filename}\")\n",
    "\n",
    "# Test prediction on a few samples\n",
    "sample_data = X_test.head(5)\n",
    "predictions = loaded_model.predict(sample_data)\n",
    "probabilities = loaded_model.predict_proba(sample_data)[:, 1]\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test[:5],\n",
    "    'Predicted': predictions,\n",
    "    'Churn_Probability': probabilities\n",
    "})\n",
    "results['Prediction_Status'] = results.apply(\n",
    "    lambda x: 'Correct' if x['Actual'] == x['Predicted'] else 'Incorrect', axis=1\n",
    ")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Verify model performance\n",
    "all_predictions = loaded_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, all_predictions)\n",
    "print(f\"\\n‚úì Model verification complete!\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nModel is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 21: Model Training Summary\n",
    "Comprehensive summary of the entire training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "summary = f\"\"\"\n",
    "‚úì TRAINING COMPLETED SUCCESSFULLY!\n",
    "\n",
    "üìä DATASET:\n",
    "   - Training samples: {len(X_train_balanced):,} (after SMOTE)\n",
    "   - Test samples: {len(X_test):,}\n",
    "   - Features: {X_train.shape[1]}\n",
    "\n",
    "ü§ñ MODELS TRAINED: 7\n",
    "   1. Logistic Regression\n",
    "   2. Decision Tree\n",
    "   3. Random Forest\n",
    "   4. XGBoost\n",
    "   5. LightGBM\n",
    "   6. Gradient Boosting\n",
    "   7. Neural Network (MLP)\n",
    "\n",
    "üèÜ BEST MODEL: {best_model_name}\n",
    "   - Test Accuracy: {tuned_metrics['Accuracy']:.4f}\n",
    "   - Precision: {tuned_metrics['Precision']:.4f}\n",
    "   - Recall: {tuned_metrics['Recall']:.4f}\n",
    "   - F1 Score: {tuned_metrics['F1_Score']:.4f}\n",
    "   - ROC-AUC: {tuned_metrics['ROC_AUC']:.4f}\n",
    "\n",
    "‚öôÔ∏è OPTIMIZATION:\n",
    "   - Method: Grid Search with 3-fold Cross-Validation\n",
    "   - Scoring Metric: F1 Score\n",
    "   - Best Parameters: {grid_search.best_params_}\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   - Model: {model_filename}\n",
    "   - Metadata: model_metadata.json\n",
    "   - Comparison: model_comparison.csv\n",
    "   - Feature Importance: feature_importance.csv\n",
    "\n",
    "üéØ KEY INSIGHTS:\n",
    "   - SMOTE was used to handle class imbalance\n",
    "   - {best_model_name} outperformed other models\n",
    "   - Model is ready for deployment and predictions\n",
    "   - Feature importance analysis reveals key churn drivers\n",
    "\n",
    "üìà NEXT STEPS:\n",
    "   1. Deploy model to production\n",
    "   2. Create prediction API/interface\n",
    "   3. Monitor model performance over time\n",
    "   4. Implement A/B testing for business impact\n",
    "   5. Regular retraining with new data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\"*100)\n",
    "print(\"‚úì MODEL TRAINING PHASE COMPLETE!\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
