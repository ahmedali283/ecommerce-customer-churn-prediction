{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Customer Churn Prediction - Data Preprocessing\n",
    "## Cell-by-Cell Guide for Google Colab\n",
    "\n",
    "This notebook covers comprehensive data preprocessing for the customer churn prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Import Required Libraries\n",
    "Run this cell first to import all necessary libraries for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('ecommerce_customer_churn_dataset.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"First 5 rows:\")\n",
    "print(\"=\"*80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Check Target Variable Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn distribution\n",
    "print(\"Churn Distribution:\")\n",
    "print(\"=\"*80)\n",
    "print(df['Churned'].value_counts())\n",
    "print(\"\\nChurn Percentage:\")\n",
    "print(df['Churned'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize churn distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='Churned', palette='Set2', ax=axes[0])\n",
    "axes[0].set_title('Churn Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Churned (0=No, 1=Yes)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "\n",
    "# Pie chart\n",
    "churn_counts = df['Churned'].value_counts()\n",
    "axes[1].pie(churn_counts, labels=['Not Churned', 'Churned'], autopct='%1.1f%%', \n",
    "            colors=['#90EE90', '#FFB6C6'], startangle=90)\n",
    "axes[1].set_title('Churn Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâš ï¸ Class Imbalance: {churn_counts[0]} (Not Churned) vs {churn_counts[1]} (Churned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Analyze Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percentage', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(missing_data)\n",
    "\n",
    "# Visualize missing values\n",
    "if len(missing_data) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=missing_data, x='Column', y='Missing_Percentage', palette='Reds_r')\n",
    "    plt.title('Missing Values by Feature', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Missing Percentage (%)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nâœ“ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"Handling Missing Values...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy 1: Fill numerical columns with median (more robust to outliers)\n",
    "numerical_cols = df_processed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numerical_cols.remove('Churned')  # Don't impute target variable\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        median_value = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_value, inplace=True)\n",
    "        print(f\"âœ“ {col}: Filled {df[col].isnull().sum()} missing values with median ({median_value:.2f})\")\n",
    "\n",
    "# Strategy 2: Fill categorical columns with mode\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        mode_value = df_processed[col].mode()[0]\n",
    "        df_processed[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"âœ“ {col}: Filled {df[col].isnull().sum()} missing values with mode ({mode_value})\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Total missing values after imputation: {df_processed.isnull().sum().sum()}\")\n",
    "print(\"âœ“ Missing value handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Detect and Analyze Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "# Analyze outliers for key numerical columns\n",
    "print(\"Outlier Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numerical_cols:\n",
    "    outlier_count, lower, upper = detect_outliers_iqr(df_processed, col)\n",
    "    outlier_pct = (outlier_count / len(df_processed)) * 100\n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'Outliers': outlier_count,\n",
    "        'Percentage': f\"{outlier_pct:.2f}%\",\n",
    "        'Lower_Bound': f\"{lower:.2f}\",\n",
    "        'Upper_Bound': f\"{upper:.2f}\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df[outlier_df['Outliers'] > 0].sort_values('Outliers', ascending=False))\n",
    "\n",
    "# Visualize outliers for key features using boxplots\n",
    "key_features = ['Age', 'Membership_Years', 'Total_Purchases', 'Average_Order_Value', 'Lifetime_Value']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(key_features):\n",
    "    sns.boxplot(data=df_processed, y=col, palette='Set3', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{col} - Outlier Detection', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Handle Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cap outliers\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap the outliers\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "print(\"Capping Outliers...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cap outliers for specific columns (Age is particularly important)\n",
    "outlier_cols = ['Age', 'Membership_Years', 'Login_Frequency', 'Session_Duration_Avg', \n",
    "                'Total_Purchases', 'Average_Order_Value', 'Lifetime_Value']\n",
    "\n",
    "for col in outlier_cols:\n",
    "    if col in df_processed.columns:\n",
    "        df_processed = cap_outliers(df_processed, col)\n",
    "        print(f\"âœ“ {col}: Outliers capped\")\n",
    "\n",
    "print(\"\\nâœ“ Outlier handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating New Features...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Purchase Frequency (purchases per year)\n",
    "df_processed['Purchase_Frequency'] = df_processed['Total_Purchases'] / (df_processed['Membership_Years'] + 0.1)\n",
    "print(\"âœ“ Created: Purchase_Frequency\")\n",
    "\n",
    "# 2. Engagement Score (composite metric)\n",
    "df_processed['Engagement_Score'] = (\n",
    "    df_processed['Login_Frequency'] * 0.3 + \n",
    "    df_processed['Session_Duration_Avg'] * 0.3 + \n",
    "    df_processed['Pages_Per_Session'] * 0.2 + \n",
    "    df_processed['Mobile_App_Usage'] * 0.2\n",
    ")\n",
    "print(\"âœ“ Created: Engagement_Score\")\n",
    "\n",
    "# 3. Shopping Behavior Score\n",
    "df_processed['Shopping_Behavior_Score'] = (\n",
    "    (100 - df_processed['Cart_Abandonment_Rate']) * 0.4 + \n",
    "    df_processed['Wishlist_Items'] * 5 + \n",
    "    df_processed['Total_Purchases'] * 2\n",
    ")\n",
    "print(\"âœ“ Created: Shopping_Behavior_Score\")\n",
    "\n",
    "# 4. Customer Value Category (based on Lifetime Value)\n",
    "df_processed['Value_Category'] = pd.cut(\n",
    "    df_processed['Lifetime_Value'], \n",
    "    bins=[0, 1000, 2000, 3000, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High', 'Premium']\n",
    ")\n",
    "print(\"âœ“ Created: Value_Category\")\n",
    "\n",
    "# 5. Recency Score (inverse of days since last purchase)\n",
    "df_processed['Recency_Score'] = 1 / (df_processed['Days_Since_Last_Purchase'] + 1)\n",
    "print(\"âœ“ Created: Recency_Score\")\n",
    "\n",
    "# 6. Customer Satisfaction Proxy\n",
    "df_processed['Satisfaction_Proxy'] = (\n",
    "    df_processed['Product_Reviews_Written'] * 10 + \n",
    "    (100 - df_processed['Returns_Rate']) * 0.5 - \n",
    "    df_processed['Customer_Service_Calls'] * 5\n",
    ")\n",
    "print(\"âœ“ Created: Satisfaction_Proxy\")\n",
    "\n",
    "# 7. Activity Level (categorical)\n",
    "df_processed['Activity_Level'] = pd.cut(\n",
    "    df_processed['Login_Frequency'],\n",
    "    bins=[0, 5, 15, 25, float('inf')],\n",
    "    labels=['Inactive', 'Low', 'Medium', 'High']\n",
    ")\n",
    "print(\"âœ“ Created: Activity_Level\")\n",
    "\n",
    "print(f\"\\nâœ“ Feature engineering complete! Dataset now has {df_processed.shape[1]} columns.\")\n",
    "print(f\"New features added: 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoding Categorical Variables...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_features = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical features to encode: {categorical_features}\\n\")\n",
    "\n",
    "# Binary encoding for Gender\n",
    "df_processed['Gender_Encoded'] = df_processed['Gender'].map({'Male': 1, 'Female': 0})\n",
    "print(\"âœ“ Gender: Binary encoded (Male=1, Female=0)\")\n",
    "\n",
    "# One-hot encoding for Country (nominal with multiple values)\n",
    "country_dummies = pd.get_dummies(df_processed['Country'], prefix='Country', drop_first=True)\n",
    "df_processed = pd.concat([df_processed, country_dummies], axis=1)\n",
    "print(f\"âœ“ Country: One-hot encoded ({len(country_dummies.columns)} new columns)\")\n",
    "\n",
    "# Label encoding for ordinal features\n",
    "le_value = LabelEncoder()\n",
    "df_processed['Value_Category_Encoded'] = le_value.fit_transform(df_processed['Value_Category'].astype(str))\n",
    "print(\"âœ“ Value_Category: Label encoded\")\n",
    "\n",
    "le_activity = LabelEncoder()\n",
    "df_processed['Activity_Level_Encoded'] = le_activity.fit_transform(df_processed['Activity_Level'].astype(str))\n",
    "print(\"âœ“ Activity_Level: Label encoded\")\n",
    "\n",
    "# One-hot encoding for Signup_Quarter\n",
    "quarter_dummies = pd.get_dummies(df_processed['Signup_Quarter'], prefix='Quarter', drop_first=True)\n",
    "df_processed = pd.concat([df_processed, quarter_dummies], axis=1)\n",
    "print(f\"âœ“ Signup_Quarter: One-hot encoded ({len(quarter_dummies.columns)} new columns)\")\n",
    "\n",
    "# Drop original categorical columns and City (too many unique values, low predictive power)\n",
    "cols_to_drop = ['Gender', 'Country', 'City', 'Signup_Quarter', 'Value_Category', 'Activity_Level']\n",
    "df_processed.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"\\nâœ“ Encoding complete! Final dataset shape: {df_processed.shape}\")\n",
    "print(f\"Dropped columns: {cols_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaling Features...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop('Churned', axis=1)\n",
    "y = df_processed['Churned']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Identify columns to scale (exclude binary and one-hot encoded features)\n",
    "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
    "onehot_cols = [col for col in X.columns if col.startswith(('Country_', 'Quarter_'))]\n",
    "encoded_cols = [col for col in X.columns if col.endswith('_Encoded')]\n",
    "\n",
    "cols_to_scale = [col for col in X.columns if col not in binary_cols + onehot_cols + encoded_cols]\n",
    "\n",
    "print(f\"\\nColumns to scale: {len(cols_to_scale)}\")\n",
    "print(f\"Binary/One-hot columns (not scaling): {len(binary_cols + onehot_cols + encoded_cols)}\")\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "\n",
    "print(\"\\nâœ“ Feature scaling complete!\")\n",
    "print(\"\\nScaled Features Sample:\")\n",
    "print(X[cols_to_scale].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting Data into Train and Test Sets...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({(len(X_train)/len(X))*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples ({(len(X_test)/len(X))*100:.1f}%)\")\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nChurn Distribution in Training Set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nChurn Distribution in Test Set:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nâœ“ Data split complete with stratification maintained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving Preprocessed Data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save train and test sets\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "print(\"âœ“ X_train.csv saved\")\n",
    "print(\"âœ“ X_test.csv saved\")\n",
    "print(\"âœ“ y_train.csv saved\")\n",
    "print(\"âœ“ y_test.csv saved\")\n",
    "\n",
    "# Save the complete preprocessed dataset\n",
    "df_processed.to_csv('preprocessed_data.csv', index=False)\n",
    "print(\"âœ“ preprocessed_data.csv saved\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names = X_train.columns.tolist()\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    for feature in feature_names:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(\"âœ“ feature_names.txt saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  - Total samples: {len(df_processed)}\")\n",
    "print(f\"  - Total features: {X_train.shape[1]}\")\n",
    "print(f\"  - Training samples: {len(X_train)}\")\n",
    "print(f\"  - Test samples: {len(X_test)}\")\n",
    "print(f\"  - Churn rate: {(y.sum()/len(y))*100:.2f}%\")\n",
    "print(\"\\nâœ“ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 14: Summary of Preprocessing Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = \"\"\"\n",
    "âœ“ COMPLETED PREPROCESSING STEPS:\n",
    "\n",
    "1. Data Loading & Exploration\n",
    "   - Loaded 50,000 customer records with 25 features\n",
    "   - Identified target variable: Churned (28.9% positive class)\n",
    "\n",
    "2. Missing Value Handling\n",
    "   - Numerical features: Filled with median values\n",
    "   - Categorical features: Filled with mode values\n",
    "   - Result: Zero missing values\n",
    "\n",
    "3. Outlier Detection & Treatment\n",
    "   - Used IQR method to identify outliers\n",
    "   - Applied winsorization (capping) instead of removal\n",
    "   - Preserved data integrity while reducing extreme values\n",
    "\n",
    "4. Feature Engineering (7 new features created)\n",
    "   - Purchase_Frequency: Measures buying activity\n",
    "   - Engagement_Score: Composite engagement metric\n",
    "   - Shopping_Behavior_Score: Shopping pattern indicator\n",
    "   - Value_Category: Customer value segmentation\n",
    "   - Recency_Score: Time since last purchase\n",
    "   - Satisfaction_Proxy: Customer satisfaction estimate\n",
    "   - Activity_Level: User activity categorization\n",
    "\n",
    "5. Categorical Encoding\n",
    "   - Binary encoding: Gender\n",
    "   - One-hot encoding: Country, Signup_Quarter\n",
    "   - Label encoding: Value_Category, Activity_Level\n",
    "   - Dropped: City (high cardinality, low predictive power)\n",
    "\n",
    "6. Feature Scaling\n",
    "   - Applied StandardScaler to continuous features\n",
    "   - Preserved binary and one-hot encoded features\n",
    "   - All features now on comparable scales\n",
    "\n",
    "7. Train-Test Split\n",
    "   - Split ratio: 80% train, 20% test\n",
    "   - Stratified sampling to maintain class balance\n",
    "   - Random state: 42 (reproducible results)\n",
    "\n",
    "8. Data Export\n",
    "   - Saved: X_train, X_test, y_train, y_test\n",
    "   - Saved: Complete preprocessed dataset\n",
    "   - Saved: Feature names for reference\n",
    "\n",
    "ðŸ“Š DATASET READY FOR MACHINE LEARNING MODELING!\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Display final feature list\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"FINAL FEATURE LIST ({len(feature_names)} features):\")\n",
    "print(\"=\"*80)\n",
    "for i, feature in enumerate(feature_names, 1):\n",
    "    print(f\"{i:2d}. {feature}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
